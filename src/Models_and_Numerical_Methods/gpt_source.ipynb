{"cells":[{"attachments":{},"cell_type":"markdown","id":"ecd39f13","metadata":{},"source":["## GPT source\n","\n","Everything we said is here.\n","We can play with anything we like, even we're far away to the real ChatGPT.\n","Most important thing is the model, see *model.py* from [nanoGPT](https://github.com/karpathy/nanoGPT).\n","It has all characteristic we discussed, with many more computer science tricks."]},{"cell_type":"code","execution_count":5,"id":"6a64c186","metadata":{"id":"6a64c186"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-05-12 00:37:50.213761\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from tqdm.notebook import trange\n","import datetime\n","\n","start = datetime.datetime.now()\n","print(start)\n","\n","\n","# hyperparameters\n","batch_size = 64  # 64 - how many independent sequences will we process in parallel?\n","block_size = 256  # 256 - what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 3e-4\n","# we will describe these 2  soon...\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","\n","# this later !!!!\n","n_embd = 384  # 384\n","n_head = 6\n","n_layer = 6\n","dropout = 0.2\n","# ------------\n","\n","torch.manual_seed(1337)\n","\n","# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","# with open('input.txt', 'r', encoding='utf-8') as f:\n","#    text = f.read()\n","\n","# nomeFile='TinyShakspeare.txt'\n","nomeFile = 'divinacommedia.txt'\n","# nomeFile='inferno.txt'\n","with open('data/'+nomeFile, 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = {ch: i for i, ch in enumerate(chars)}\n","itos = {i: ch for i, ch in enumerate(chars)}\n","# encoder: take a string, output a list of integers\n","def encode(s): return [stoi[c] for c in s]\n","# decoder: take a list of integers, output a string\n","def decode(l): return ''.join([itos[i] for i in l])\n","\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data))  # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","\n","@torch.no_grad()\n","# later\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# super simple bigram model\n","\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","        # these only later, now don't think about it.....\n","\n","        # self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        # self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        # self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        # self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","\n","        logits = self.token_embedding_table(idx)  # (B,T,C)\n","\n","        # these only later, now don't think about it.....\n","\n","        # tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        # pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        # x = tok_emb + pos_emb # (B,T,C)\n","        # x = self.blocks(x) # (B,T,C)\n","        # x = self.ln_f(x) # (B,T,C)\n","        # logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :]  # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1)  # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n","        return idx\n","\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(\n","            torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # input of size (batch, time-step, channels)\n","        # output of size (batch, time-step, head size)\n","        B, T, C = x.shape\n","        k = self.key(x)   # (B,T,hs)\n","        q = self.query(x)  # (B,T,hs)\n","        # compute attention scores (\"affinities\")\n","        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n","        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n","        wei = wei.masked_fill(\n","            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n","        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x)  # (B,T,hs)\n","        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n","        return out\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","\n","class GPTLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(\n","            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","        # better init, not covered in the original GPT video, but important, will cover in followup video\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n","        pos_emb = self.position_embedding_table(\n","            torch.arange(T, device=device))  # (T,C)\n","        x = tok_emb + pos_emb  # (B,T,C)\n","        x = self.blocks(x)  # (B,T,C)\n","        x = self.ln_f(x)  # (B,T,C)\n","        logits = self.lm_head(x)  # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :]  # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1)  # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n","        return idx\n"]},{"cell_type":"code","execution_count":null,"id":"67df18ab","metadata":{"id":"67df18ab","outputId":"ccdb86a3-ca74-46d5-e378-4c0324f97095"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.048637 M parameters\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fa4dee233a446bcab5d890ee250b9f1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["step 0: train loss 3.6072, val loss 3.6067\n","step 500: train loss 2.3330, val loss 2.3330\n","step 1000: train loss 2.1680, val loss 2.1697\n","step 1500: train loss 2.0893, val loss 2.0984\n","step 2000: train loss 2.0345, val loss 2.0324\n","step 2500: train loss 1.9687, val loss 1.9727\n","step 3000: train loss 1.9257, val loss 1.9188\n","step 3500: train loss 1.8872, val loss 1.8758\n","step 4000: train loss 1.8571, val loss 1.8512\n","step 4500: train loss 1.8303, val loss 1.8295\n","step 5000: train loss 1.8141, val loss 1.8095\n","step 5500: train loss 1.7981, val loss 1.7945\n","step 6000: train loss 1.7831, val loss 1.7787\n","step 6500: train loss 1.7783, val loss 1.7713\n","step 7000: train loss 1.7696, val loss 1.7679\n","step 7500: train loss 1.7649, val loss 1.7602\n","step 8000: train loss 1.7542, val loss 1.7510\n","step 8500: train loss 1.7420, val loss 1.7356\n","step 9000: train loss 1.7332, val loss 1.7291\n","step 9500: train loss 1.7321, val loss 1.7264\n","step 10000: train loss 1.7271, val loss 1.7213\n","step 10500: train loss 1.7168, val loss 1.7171\n","step 11000: train loss 1.7081, val loss 1.7113\n","step 11500: train loss 1.7136, val loss 1.6983\n","step 12000: train loss 1.7045, val loss 1.7043\n","step 12500: train loss 1.6967, val loss 1.6975\n","step 13000: train loss 1.7000, val loss 1.6927\n","step 13500: train loss 1.6983, val loss 1.6956\n","step 14000: train loss 1.6994, val loss 1.6934\n","step 14500: train loss 1.6886, val loss 1.6826\n","step 15000: train loss 1.6815, val loss 1.6812\n","step 15500: train loss 1.6900, val loss 1.6803\n","step 16000: train loss 1.6862, val loss 1.6769\n","step 16500: train loss 1.6791, val loss 1.6792\n","step 17000: train loss 1.6811, val loss 1.6754\n","step 17500: train loss 1.6804, val loss 1.6706\n","step 18000: train loss 1.6748, val loss 1.6707\n","step 18500: train loss 1.6749, val loss 1.6663\n","step 19000: train loss 1.6714, val loss 1.6704\n","step 19500: train loss 1.6706, val loss 1.6681\n","step 20000: train loss 1.6652, val loss 1.6688\n","step 20500: train loss 1.6632, val loss 1.6629\n","step 21000: train loss 1.6627, val loss 1.6594\n","step 21500: train loss 1.6592, val loss 1.6584\n","step 22000: train loss 1.6590, val loss 1.6530\n","step 22500: train loss 1.6574, val loss 1.6576\n","step 23000: train loss 1.6623, val loss 1.6543\n","step 23500: train loss 1.6573, val loss 1.6534\n","step 24000: train loss 1.6546, val loss 1.6553\n","step 24500: train loss 1.6506, val loss 1.6462\n","step 25000: train loss 1.6549, val loss 1.6521\n","step 25500: train loss 1.6542, val loss 1.6514\n","step 26000: train loss 1.6512, val loss 1.6473\n","step 26500: train loss 1.6494, val loss 1.6458\n","step 27000: train loss 1.6507, val loss 1.6484\n","step 27500: train loss 1.6426, val loss 1.6477\n","step 28000: train loss 1.6458, val loss 1.6441\n","step 28500: train loss 1.6440, val loss 1.6410\n","step 29000: train loss 1.6410, val loss 1.6424\n","step 29500: train loss 1.6427, val loss 1.6411\n","step 30000: train loss 1.6404, val loss 1.6380\n","step 30500: train loss 1.6438, val loss 1.6413\n","step 31000: train loss 1.6426, val loss 1.6308\n","step 31500: train loss 1.6388, val loss 1.6365\n","step 32000: train loss 1.6443, val loss 1.6388\n","step 32500: train loss 1.6374, val loss 1.6364\n","step 33000: train loss 1.6300, val loss 1.6300\n","step 33500: train loss 1.6381, val loss 1.6335\n","step 34000: train loss 1.6325, val loss 1.6318\n","step 34500: train loss 1.6352, val loss 1.6343\n","step 35000: train loss 1.6321, val loss 1.6320\n","step 35500: train loss 1.6333, val loss 1.6313\n","step 36000: train loss 1.6343, val loss 1.6330\n","step 36500: train loss 1.6312, val loss 1.6308\n","step 37000: train loss 1.6254, val loss 1.6270\n","step 37500: train loss 1.6313, val loss 1.6329\n","step 38000: train loss 1.6270, val loss 1.6214\n","step 38500: train loss 1.6276, val loss 1.6235\n","step 39000: train loss 1.6365, val loss 1.6355\n","step 39500: train loss 1.6258, val loss 1.6259\n","step 40000: train loss 1.6290, val loss 1.6278\n","step 40500: train loss 1.6267, val loss 1.6241\n","step 41000: train loss 1.6219, val loss 1.6142\n","step 41500: train loss 1.6232, val loss 1.6223\n","step 42000: train loss 1.6246, val loss 1.6216\n","step 42500: train loss 1.6232, val loss 1.6196\n","step 43000: train loss 1.6226, val loss 1.6235\n","step 43500: train loss 1.6203, val loss 1.6186\n","step 44000: train loss 1.6207, val loss 1.6256\n","step 44500: train loss 1.6166, val loss 1.6207\n","step 45000: train loss 1.6206, val loss 1.6189\n","step 45500: train loss 1.6182, val loss 1.6212\n","step 46000: train loss 1.6276, val loss 1.6177\n","step 46500: train loss 1.6245, val loss 1.6138\n","step 47000: train loss 1.6132, val loss 1.6127\n","step 47500: train loss 1.6179, val loss 1.6177\n","step 48000: train loss 1.6178, val loss 1.6136\n","step 48500: train loss 1.6221, val loss 1.6130\n","step 49000: train loss 1.6204, val loss 1.6149\n","step 49500: train loss 1.6224, val loss 1.6203\n","step 49999: train loss 1.6231, val loss 1.6170\n"," sugne alla nche conorà del già incommi per avocio anufé l moli al perdetina a conlo per lue fece io figliamò dò le ciò qui non colà per le lo ommagi a passe innanzi dicimo per l nozza vera pribiro dico puota fumar guiva mia graroce mistri batto perversì lo poppo lima che un gelva cistremi cond ecchi manno e quando e suofbiglia di quetter moggli sì lo sanza ten come del su isponda dicer si testi innar dolce altretïente anna al lue preconse sovrace non cerso con la niconten perlo cramicò de se olt\n"]},{"data":{"text/plain":["10001"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["model = BigramLanguageModel(vocab_size)\n","\n","max_iters = 50000\n","model = GPTLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in trange(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(\n","            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","open('more.txt', 'w').write(\n","    decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"]},{"cell_type":"code","execution_count":null,"id":"b01b15be","metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), 'divinacommedia.pt')\n","\n","end = datetime.datetime.now()\n","print(end)\n","print(end - start)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":5}
